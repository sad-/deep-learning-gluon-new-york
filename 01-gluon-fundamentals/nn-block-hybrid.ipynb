{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"support/neuralnetwork.gif\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now let's look how to create neural networks in Gluon. In addition the NDArray package (`nd`) that we just covered, we now will also import the neural network `nn` package from `gluon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create your first neural network layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's start with a dense layer with 2 output units.\n",
    "The None is because there hasn't been any data passed in yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"support/fullyconnected.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Dense(2, activation=\"relu\")\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Then initialize its weights with the default initialization method, which draws random values uniformly from $[-0.7, 0.7]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "32"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer.initialize(mx.init.Xavier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Then we do a forward pass with random data. We create a $(3,4)$ shape random input `x` and feed into the layer to compute the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Forward with input `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "34"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "x = nd.random.uniform(low=-1, high=1, shape=(N, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "34"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output = layer(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Inferred shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As can be seen, the layer's input limit of 2 produced a $(3,2)$ shape output from our $(3,4)$ input. Note that we didn't specify the input size of `layer` before (though we can specify it with the argument `in_units=4` here), the system will automatically infer it during the first time we feed in data, create and initialize the weights. So we can access the weight after the first forward pass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chain layers into a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's first consider a simple case that a neural network is a chain of layers. During the forward pass, we run layers sequentially one-by-one. The following code implements a famous network called [LeNet](http://yann.lecun.com/exdb/lenet/) through `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    # Add a sequence of layers.\n",
    "    net.add(\n",
    "        nn.Conv2D(channels=6, kernel_size=(5, 5), activation='relu'),\n",
    "\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120, activation=\"relu\"),\n",
    "        nn.Dense(84, activation=\"relu\"),\n",
    "        nn.Dense(10)\n",
    "    )\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mx.viz.plot_network(net(mx.sym.var('data')), \n",
    "                    shape={\"data\":(1, 1, 28, 28)},\n",
    "                    node_attrs={\"shape\":\"oval\",\"fixedsize\":\"False\"},\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<!--Mention the tuple option for kernel and stride as an exercise for the reader? Or leave it out as too much info for now?-->\n",
    "\n",
    "The usage of `nn.Sequential` is similar to `nn.Dense`. In fact, both of them are subclasses of `nn.Block`. The following codes show how to initialize the weights and run the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "# Input shape is (batch_size, color_channels, height, width)\n",
    "x = nd.random.uniform(shape=(4, 1, 28, 28))\n",
    "y = net(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can use `[]` to index a particular layer. For example, the following\n",
    "accesses the 1st layer's weight and 6th layer's bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Specific layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\"First Conv2D layer weight shape {}\".format(net[0].weight.data().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a neural network flexibly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In `nn.Sequential`, MXNet will automatically construct the forward function that sequentially executes added layers.\n",
    "Now let's introduce another way to construct a network with a flexible forward function.\n",
    "\n",
    "To do it, we create a subclass of `nn.Block` and implement two methods:\n",
    "\n",
    "- `__init__` create the layers\n",
    "- `forward` define the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class MixMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Run `nn.Block`'s init method\n",
    "        super(MixMLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.features = nn.Sequential()\n",
    "            # Already within a name scope, no need to create\n",
    "            # another scope.\n",
    "            self.features.add(\n",
    "                nn.Dense(3, activation='relu'),\n",
    "                nn.Dense(4)\n",
    "            )\n",
    "            self.output = nn.Dense(5)\n",
    "    def forward(self, x):\n",
    "        y = nd.relu(self.features(x))\n",
    "        print(\"Features\", y)\n",
    "        return self.output(y)\n",
    "\n",
    "net2 = MixMLP()\n",
    "net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the sequential chaining approach, we can only add instances with `nn.Block` as the base class and then run them in a forward pass. In this example, we used `print` to get the intermediate results and `nd.relu` to apply relu activation. So this approach provides a more flexible way to define the forward function.\n",
    "\n",
    "The usage of `net` is similar as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net2.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = nd.random.uniform(shape=(2,2))\n",
    "out = net2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, let's access a particular layer's weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Weight access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net2.features[1].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast, portable neural networks with `hybrid`\n",
    "<br>\n",
    "<center><img src=\"support/fast.gif\" width=300><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First let's understand imperative and symbolic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imperative Pseudofunction\n",
    "```\n",
    "def our_function(A, B, C, D):\n",
    "    # Compute some intermediate values\n",
    "    E = basic_function1(A, B)\n",
    "    F = basic_function2(C, D)\n",
    "    \n",
    "    # Produce the thing you really care about\n",
    "    G = basic_function3(E, F)\n",
    "    return G\n",
    "    \n",
    "# Load up some data\n",
    "W = some_stuff()\n",
    "X = some_stuff()\n",
    "Y = some_stuff()\n",
    "Z = some_stuff()\n",
    "    \n",
    "result = our_function(W, X, Y, Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symbolic Pseudofunction\n",
    "\n",
    "```\n",
    "# Placeholders to stand in for real data\n",
    "A = placeholder() \n",
    "B = placeholder()\n",
    "C = placeholder()\n",
    "D = placeholder()\n",
    "\n",
    "# Compute some intermediate values\n",
    "E = symbolic_function1(A, B)\n",
    "F = symbolic_function2(C, D)\n",
    "    \n",
    "# Produce the thing you really care about\n",
    "G = symbolic_function3(E, F)\n",
    "    \n",
    "our_function = library.compile(inputs=[A, B, C, D], outputs=[G])   \n",
    "    \n",
    "# Load up some data\n",
    "W = some_stuff()\n",
    "X = some_stuff()\n",
    "Y = some_stuff()\n",
    "Z = some_stuff()\n",
    "    \n",
    "result = our_function(W, X, Y, Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tradeoffs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imperative Programs Tend to be More Flexible\n",
    "* familiar style faster debugging, means you get to try out more ideas.\n",
    "* the catch is that imperative programs are *comparatively* slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Symbolic Programs Tend to be More Efficient\n",
    "* memory efficiency via reuse for intermediate results/speed optimizations via operator folding\n",
    "* the catch is the tricky indirection of working with placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting the best of both worlds with MXNet Gluon's `HybridBlock`s\n",
    "\n",
    "\n",
    "**All of MXNet's predefined layers are HybridBlocks.** This means that any network consisting entirely of predefined MXNet layers can be compiled and run at much faster speeds by calling ``.hybridize()``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HybridSequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    # construct a MLP\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(256, activation=\"relu\"))\n",
    "        net.add(nn.Dense(128, activation=\"relu\"))\n",
    "        net.add(nn.Dense(2))\n",
    "    # initialize the parameters\n",
    "    net.collect_params().initialize()\n",
    "    return net\n",
    "\n",
    "# forward\n",
    "x = nd.random_normal(shape=(1, 512))\n",
    "net = get_net()\n",
    "print('=== net(x) ==={}'.format(net(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net.hybridize()\n",
    "print('=== net(x) ==={}'.format(net(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Performance\n",
    "Compare the performance before and after hybridizing \n",
    "by measuring the time it takes to make 1000 forward passes through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def bench(net, x):\n",
    "    mx.nd.waitall()\n",
    "    start = time()\n",
    "    for i in range(1000):\n",
    "        y = net(x)\n",
    "    mx.nd.waitall()\n",
    "    return time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = get_net()\n",
    "print('Before hybridizing: %.4f sec'%(bench(net, x)))\n",
    "net.hybridize()\n",
    "print('After hybridizing: %.4f sec'%(bench(net, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## let's dive deeper into how `hybridize` works.\n",
    "* Recall, Gluon networks are composed of Blocks each of which subclass `gluon.Block`\n",
    "* For hybrid networks, we have `gluon.HybridBlock`\n",
    "* To define a `HybridBlock`, we have to define a`hybrid_forward` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HybridBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "class Net(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.fc1 = nn.Dense(256)\n",
    "            self.fc2 = nn.Dense(128)\n",
    "            self.fc3 = nn.Dense(2)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        # F is a function space that depends on the type of x\n",
    "        # If x's type is NDArray, then F will be mxnet.nd\n",
    "        # If x's type is Symbol, then F will be mxnet.sym\n",
    "        print('type(x): {}, F: {}'.format(\n",
    "                type(x).__name__, F.__name__))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.collect_params().initialize()\n",
    "x = nd.random_normal(shape=(1, 512))\n",
    "print('=== 1st forward ===')\n",
    "y = net(x)\n",
    "print('=== 2nd forward ===')\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net.hybridize()\n",
    "print('=== 1st forward ===')\n",
    "y = net(x)\n",
    "print('=== 2nd forward ===')\n",
    "y = net(x)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
